PRACTICALMetabolomics using Machine Learning: Part BTuesday, 14th January 2025Maria Anastasiadi (m.anastasiadi@cranfield.ac.uk)IntroductionSupport Vector Machines (SVM) classifiers can be adapted for use with any type of learning task and have grown in popularity recently, because of their good performance but also because the availability of good libraries which hide the somewhat complex maths required. In this practical, you will apply an SVM algorithm to the same dataset as the previous practical and evaluate its performance.Kernel choice, especially nonlinear kernels, may add additional dimensions to the data, creating separation even when the data appears inseparable. This “kernel trick” allows SVM to learn concepts that were not explicitly measured in the original data.  For this practical we require the following libraries (remember to install any missing packages first):library(caret)library(gmodels)library(kernlab)library(LiblineaR)Data LoadingTo load the data into R, you can use the read.table() R function:In order to combine all rows from EnoseAllSamples.csv and SensoryAllSamples.csv, you can use the merge() function.Once loaded, the data should look familiar. We have already examined/explored this dataset during the previous practical (k-NN & Decision Trees). However, you’ll need to re-partition the dataset into a training set and a test set.Data PreparationUse the same approach as last time to partition the data into a training set (70% of observations) and a test set (30% of observations). The selection of observations for each set should be done randomly. However, all classes should be represented fairly in the two sets. The createDataPartition() function from the caret package is the easiest way to achieve this. Remember to set seed as you did in the previous practical for reproducibility and to be able to compare the performance of the different algorithms. Please ensure you preserve the correspondence between numerical (predictor) and categorical (response) data. You can do this easily with createDataPartition() as it returns an array of response vector positions as the partition. You can then use these indexes to produce both the training and the test set. For instance:trainSet <- AllData[trainIndex,]; testSet <- AllData[-trainIndex,]Model TrainingWe will use the very versatile train() function from the caret package to train our model. In this practical, our aim is to train a spoilage classifier based on the enose sensor responses (DF1 to DF8). For more background information on the dataset, please see the previous practical's notes. The train() function can be used to train many different classification and regression models, and the specific method to use should be provided as the method parameter. For different variants of SVM, you can check the following link: Support_Vector_Machines_VariantsNote that you may need to load additional required packages to use some of the methods. We can begin by using the "Support Vector Machines with Linear Kernel" method, and then print the result:model.svm <- train(sensory ~ ., data = trainSet, method="svmLinear2")model.svmNotice that the model reports "Accuracy" metrics for different value of a "cost" (C) parameter. The train() function attempts to tune model parameters and find the combination of parameters that gives the highest accuracy. The values used for tuning can be controlled using the tuneGrid and tuneLength parameters. The training data is randomly resampled during this process (using a bootstrapping method by default), so the resulting parameters may differ slightly between runs.We cannot visualise the hyperplane from this model, as there are too many dimensions.Model EvaluationTo evaluate your model, you will need to use the predict() function to obtain a vector of classes predicted for the test set by your model:predicted <- predict(model.svm, testSet)a) Use the CrossTable() function from the gmodels package to view the counts of true/false positives/negatives.cross.table <- CrossTable(testCl, predicted, prop.chisq=FALSE, prop.t=FALSE, prop.c=FALSE, prop.r=FALSE)NOTE: here we give the predicted values as argument unlike knn where we give the model. The reason is that since knn does not produce a model and the predictions are included in the “model” produced. In the case of other algorithms we need to use the vector with the predicted values. You can also use confusionMatrix() from the caret package, which makes it easy to obtain the prediction accuracy programmatically confusion.matrix <- confusionMatrix(predicted, testCl, positive="3")b) Question: What is the prediction accuracy of your model? How does it compare to the knn and decision tree model accuracy from the previous practical?Task1: So far, we have only used one random data partition to test the performance of the SVM algorithm. However, given a different random data partition, would the accuracy of the predictions change? How sensitive is our model performance on the sampling of the original data?To investigate, use the createDataPartition() function to generate 100 random data partitions (Hint: you can use the times parameter of createDataPartition() to do this). Fill in the missing argumentstrainIndex1 <- createDataPartition(, p = .7,                                       list = FALSE,                                       times =)Then write a for loop which will train an SVM model, predict the classes of the test set, and evaluate the success rate (accuracy) of each run for each of the 100 partitions. # Set the vector “accuracies”accuracies<-c()# Fill in the loop with the missing argumentsfor (i in 1:100){    trainSet <- AllData[trainIndex1[,i],]    testSet <-    testClass <-     model_svm <- train(sensory ~ ., data=, method=)    kernel.predicted <- predict()    kernel.confusion.matrix <- confusionMatrix(kernel.predicted,     testClass, positive="3")    current.accuracy <- kernel.confusion.matrix$overall[1]    accuracies <-}NOTE: Make sure you store the accuracy values from each iteration in a vector. You can append values like this: accuracies <- c(accuracies, current.accuracy)Finally plot a histogram of the accuracy measurement, which will allow you to assess the accuracy distribution:hist(accuracies, xlab="model accuracy", main="SVM classifier prediction accuracy")Question: Is there a distinct peak? If so, what does it represent?Model ImprovementTo improve our SVM model, we can evaluate the use of different kernels. You can use any methods listed in the caret documentation link above, but for the purposes of the current practical we will focus on four:svm.methods <- c('lssvmRadial', 'lssvmPoly', 'svmRadial', 'svmLinear3')Task 2 [Optional]: Create a function named accuracy.m(), which applies a loop to run 100 models for each kernel and stores the accuracies for each kernel in a vector called mean.accuracies.This function takes two arguments:data = the dataset; svm.method.name = one of the svm.methods ('lssvmRadial', 'lssvmPoly', 'svmRadial', 'svmLinear3')Complete the function body and test that the function works. accuracy.m <-function(data, svm.method.name){  accuracies.m <-  for(j in 1:100){    classes<-data[,ncol(data)]    trainIndex <- createDataPartition()    trainIndex<-as.matrix(trainIndex)    trainSet <- data[trainIndex,]    testSet <-     testC <-    model_svm <- train(sensory ~ ., data=, kernel=)    kernel.predicted <- predict(model_svm, testSet)    kernel.confusion.matrix <- confusionMatrix(kernel.predicted,    testC, positive="3")    current.accuracy <- kernel.confusion.matrix$overall[1]    accuracies.m <-      }  return(accuracies.m)Task 3 [Optional]: Run a loop for each element in the svm.methods vector and calculate the mean accuracy for each method. Store the results in a mean.accuracies vector. Finally, plot the respective mean accuracies with barplot().#Fill in the missing argumentsmean.accuracies <-for(i in svm.methods){  b <-accuracy.m(AllData, svm.method.name =i)  m.mean.accuracy <-mean(b)  mean.accuracies <-}Question: Which kernel produces the highest classification success rates?Make sure to take a look at the model parameters that are tuned for each method.Please note that we can also apply a grid search to automatically tune the model. We could also try to improve the accuracy by applying scaling, as svm is also a distance-based algorithm. In fact, it is good practice to try scaling irrespective of the algorithm applied, but for some types of algorithms it is highly recommended.  MSc Applied Bioinformatics		ML for Metabolomics2