PRACTICALRandom ForestsThursday 12th February 2026Maria Anastasiadi (m.anastasiadi@cranfield.ac.uk)IntroductionRandom Forests is an ensemble-based method which focuses only on ensembles of decision trees and makes use of bootstrap aggregating (bagging) to create many training sets from the original dataset using sampling with replacement.  Random Forest adds an additional layer of randomness to bagging. In addition to constructing each tree using a different bootstrap sample of the data, random forests change how the classification or regression trees are constructed. In standard trees, each node is split using the best split among all variables. In a random forest, each node is split using the best split among a subset of predictors randomly chosen at that node. This somewhat counterintuitive strategy turns out to perform very well compared to many other classifiers, including discriminant analysis, support vector machines and neural networks, and is robust against overfitting. In this practical you will tune, train and validate a model using the Random Forest algorithm included in mlr3verse, which like caret is an interface to a large number of classification and regression techniques, including machine-readable parameter descriptions. The mlr3 package contained within mlr3verse provides the base functionality and the fundamental building blocks for machine learning. There is also a set of other packages which together with mlr3 form the mlr3verse ecosystem, and they are used to extend capabilities of mlr3, for preprocessing, pipelining, visualizations, additional learners, additional task types, and so. You can find more information on https://mlr3book.mlr-org.com.DatasetIn today’s practical you will work with the dataset Zoo which is included in the library mlbench and the data has been taken from the UCI Repository Of Machine Learning Databases. Zoo is a simple dataset containing 17 (mostly logical) variables on 101 animals. The data frame has 17 columns (16 predictor variables and one target variable): hair, feathers, eggs, milk, airborne, aquatic, predator, toothed, backbone, breathes, venomous, fins, legs, tail, domestic, catsize, type. Most variables are logical and indicate whether the corresponding animal has the corresponding characteristic or not. Your task is to tune, train and cross-validate a classification model for predicting what group of animals the samples belong to (mammals, birds, reptiles, fish, amphibian, insect, mollusc et. al) using the Random Forest algorithm. You will need to install and load the following packages: mlr3verse, ranger, tidyverse, mlbench, mlr3viz, mlr3learners, paradox, bbotk.   Data Loading & ExplorationTo load the data into R, type data(Zoo). Data PreparationBefore continuing building the model you first need to convert the logical variables into factors as mlr3 does not accept logical predictors. You can do so by using the function mutate_all().Use Zoo as the first argument and name the new dataframe zooD .zooD <- mutate_all(Zoo, as.factor)head(zooD)Check the proportion of each class in the dataset.round(prop.table(table(zooD$type)) * 100, digits = 1)Next remove the samples with number of legs 5 or 8 which are underrepresented. Name the new dataset zoo1. a <- which(zooD$legs=="8")b <- which(zooD$legs=="5")zoo1 <- zooD[-c(a,b),]Model Building1. Create Model TaskFirst you need to define the task (call it task_zoo) using the function as_task_classif(). task_zoo = as_task_classif(type ~ ., data = zoo1)We provide two arguments to the function: data = zoo1 and target = type~. which tells R to use all the predictors to build the model.  You can still retrieve the data from the task_zoo object by typing: task_zoo$data()You can inspect the frequency of each class by creating a barchart with class counts:autoplot(task_zoo)Before specifying which algorithm to use for building the model we can inspect the list of available algorithms as follows:mlr_learnersThis will print the list of algorithms in the console, which include all the algorithms included in mlr3 and the extension packages. We are interested in applying classif.ranger which is an algorithm for performing Random Forests available with the package ranger. To do so we need to set the learner:learner = lrn("classif.ranger")2. Model TrainingWe can use this learner to create a first Random Forest (RF) model and check its performance. First we partition the dataset into train and test set using the function partition(). set.seed(4)split = partition(task_zoo, ratio = 0.67)Then proceed with model training. learner$train(task_zoo, split$train)You can check the trained model as follows:learner$model3.  Model Testing The next step is to validate your model with the independent test set and check the classification accuracy. prediction = learner$predict(task_zoo, split$test)#classification accuracymeasure = msr("classif.acc")prediction$score(measure)Question: What is the model performance? Take note of the prediction accuracy. It should be >0.9.To inspect the performance of the model for each class we can construct a confusion matrix:prediction$confusionand a stacked barplotautoplot(prediction)4. Model TunningOur first attempt to build a RF model gave good results. However, in real life situations, we may want to optimise our model by tuning the hyperparameters. Machine learning algorithms usually have parameters and hyperparameters. Parameters are what we might think of as model coefficients or weights. Hyperparameters are configured by the user and determine how the model will fit its parameters. We can check what hyperparameters our algorithm of choice has by typing: learner$param_set$ids()From the list of parameters printed in the console the following four hyperparameters are the most important to consider when tuning a RF model:* num.trees, the number of individual trees in the forest* mtry, the number of features to randomly sample at each node* min.node.size, the minimum number of cases allowed in a leaf (terminal node)* max.depth, the maximum number of leaves allowed1) Create the hyperparameter space using the function lrn()and define the lower and upper values for each of the above hyperparameters. For the number of trees we set between 200 and 500. For the number of features at each node we set mtry between 2-12 (we want mtry to be large enough to capture the trends in the dataset but less than the total number of features). For min.node.size allow two cases in each leaf, and allow <20 nodes in each tree so as not to grow too much, but also avoid under-fitting. learner1 = lrn("classif.ranger",              num.trees = to_tune(200, 500),              mtry  = to_tune(2, 12),              min.node.size = 2,              max.depth = 20		    )2) Next define an evaluation method, i.e., a resampling strategy and a performance measure, using the: rsmp()and msr() function respectively:resampling = rsmp("cv", folds = 3)measure = msr("classif.acc")3) Finally, we have to specify the budget available for tuning. This is an important step, as exhaustively evaluating all possible hyperparameter configurations is usually not feasible. The most commonly used terminators are those that stop tuning after a certain time ("run_time") or number of evaluations ("evals"). Here set number of evaluations = 20 using the function trm() .terminator = trm("evals", n_evals = 20)4) Define a new task called task_train:train_Set<-zoo1[split$train,]task_train<-as_task_classif(type ~ ., data = train_Set) 5) Now put everything together into a mlr3tuning::TuningInstanceSingleCrit with the mlr3tuning::ti() function. instance = ti(task = task_train,  			learner = learner1,  			resampling = resampling,  			measures = measure,  			terminator = terminator			)instance6) By looking at the instance object we can see that the tuning instance specifies the tuning space. We still haven’t performed any tuning. To start the tuning, we still need to select how the optimization should take place. Two of the most popular methods is grid search ("grid_search") and random search ("random_search"). Grid search discretizes the range of each hyperparameter and exhaustively evaluates each combination. Random search simply samples randomly hyperparmeters from a uniform distribution. For our example, we will use a simple grid search with a grid resolution of 5 with the tnr function:tuner = tnr("grid_search", resolution = 5, batch_size = 4)7) Finally, we initiate the tuning process:tuner$optimize(instance)5. Build Final ModelNow we can build a new model using the tuned hyperparameters from the previous step. To do so type:learner1$param_set$values = instance$result_learner_param_valslearner1$train(task_train)learner1$model6. Test Final ModelNow test the new model performance with the independent test set. Set accuracy as performance measure.  prediction = learner1$predict(task_zoo, split$test)#classification accuracymeasure = msr("classif.acc")prediction$score(measure)Finally, create a confusion matrix and a stacked barchart as previously:prediction$confusionautoplot(prediction)Can you see any improvements to the model performance? Optional Task: You can use the same strategy as previously to tune, train and optimise a RF model on the “PimaIndiansDiabetes2” dataset also included in the mlbench package.  This is a dataset containing the diabetes status of 678 individuals denoted as “pos” or “neg” along with 8 predictor (explanatory) variables including serum glucose levels, age, blood pressure etc. Note: The PimaIndiansDiabetes2 dataset has a few NAs. Make sure to remove them before attempting the task. MSc Applied Bioinformatics		ML for Metabolomics2