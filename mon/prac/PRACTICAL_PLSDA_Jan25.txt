PRACTICALQualitative Metabolomics:PLS-DAMonday, 13th January 2025Maria Anastasiadi (m.anastasiadi@cranfield.ac.uk)1. IntroductionMetabolomics is the study of chemical profiles, in this case, metabolites, that are generated during the process of metabolism (the chemical reactions that occur in all forms of life). Metabolites tend to be the end products of cellular processes, which occur in plants, animals, and humans. Regarding the latter, they are found in biological cells, tissues, and organs. Acquiring the metabolic profile can indicate the condition of a cell, such as whether it is normal or abnormal. These profiles can be acquired by measuring samples collected from blood, urine, plant tissues microbial samples etc. Some commonly employed analytical techniques are Fourier Transform infrared spectroscopy (FT-IR), liquid chromatography mass spectrometry (LC-MS), gas chromatography mass spectrometry (GC-MS) and nuclear magnetic resonance (NMR).Today’s practical, will focus on multivariate classification using PLS-DA. Multivariate classification is termed a supervised method because it involves the use of “training sets” where the outcomes are already known so that unknown samples can then be “classed” into groups by regression on to the appropriate localised models. PLS-DA (partial least squares discriminant analysis) is a commonly employed method for multivariate classification. It generates an overall model to account for all of the groups. It also calculates the probability that a new sample belongs to a particular class. It can thus be employed if only a qualitative determination is required, for example whether the sample is classed as diseased or healthy.In the first part of the practical, you will try to build a classification model to identify different fungi species based on their enose profile and use this model to predict the class of two newly acquired samples. In the second part, you will focus on data acquired via GC-MS. We will be working with data acquired from the faecal samples of patients suffering from Crohn’s disease (CD) vs faecal samples from healthy patients. We will be looking to see if we can classify three samples via partial least squares discriminant analysis (PLS-DA) to determine whether these samples are healthy (class value of 1) or indicate the presence of the disease, in this case CD (class value of 2). It is recommended to work with R.4.4.0 or higher using the following packages: mixOmics, rgl, matlab, R.matlab, ptw, dplyr.In case mixOmics is not installed, you can install it via the console by typing:install.packages("BiocManager")BiocManager::install("mixOmics")2. Multivariate classification using PLS-DA and enose data.You have been supplied with the dataset data2.csv, which contains the volatile profile from different pathogenic fungi acquired via an electronic nose (e-nose).  The current samples are type cultures of four Trichophyton species viz T mentagrophytes (M), T rubrum (R), T verrucosum (E) and T violaecuem (I) along with a set of controls (blank agar (B)). There is an additional column in the dataset called Class ID which relates each fungi species to a different class (1-5).  The objective of this exercise is to split the dataset into a training set and a test set. Then you will attempt to build a model using the training set and validate it with the test set, to see if the model can predict the correct class for the “unknown” samples.Import the dataset using the read.table() function and then extract the column and row headers using the colnames() and rownames() functions to respectively create two character vectors: Sensors and Samples. From the numerical matrix (named X), you need to:Extract the class ID column into a column vector (class1)Create a new matrix (RESP) from X which does not contain the class ID column. 2.1 Exploratory data analysis with PCAPrior to classification, it is common to perform principal components analysis (PCA). This can indicate if there are potential outliers and whether variables (columns) in the data matrix can be omitted. You can perform pca using the pca() function in the mixOmics package. You can select the number of principal components and whether to scale the original data or not. E.g.pca.enose <- pca(RESP, ncomp = 6, scale = TRUE)Then to create a pca scatterplot type:#define class vectorclass1 = as.factor(class1)x11()plotIndiv(pca.enose, ind.names = Samples,           group = class1, style = "lattice") # you can also                 #select style = "ggplot2"Though 2D plots are common, e.g. PC2 vs PC1, one can easily create 3D plots in R. In a 2D plot, two samples may appear to be similar as they are close together; however, a 3D plot (e.g. PC3 vs PC2 vs PC1) may show these samples to be very distinct from one another. To generate a 3D PCA plot simply change style = “3d”.You should attain the following GUI similar to that shown in Figure 1. This is an interactive plot and you can move it at different angles. Can you observe any trends? Figure 1. The generated GUI displaying a 3D PCA plot for the fungi dataset. You can also generate a biplot using the biplot() function to see which variables drive the separation in the pca plot. Extract the variance for each PC from the resulting model after you perform PCA and display the % variance in the biplot as well. (Hint: You can find the variance explained by each PC in the object prop_expl_var within the pca.enose model).biplot(pca.enose, xlab=paste("PC1 ", Var1, "%"),         ylab=paste("PC2 ", Var2, "%"),group = class1,        col.per.group= c("red", "blue", "orange", "green", "gray"))What conclusions can you draw? Have samples belonging to the same group clustered together? Which variables drive the separation?2.2 Multivariate classification with PLS-DAYou will use the mixOmics package to perform PLS-DA. First create your training set using rows 1:20 from the RESP matrix and call it train1. Do the same for the class1 vector: extract elements 1:20 into a new vector called class (ensure it is a factor as well).  Then create your test set using rows 21:25 from the RESP matrix and call it test1.  Take the last 5 rows from your class1 vector to create your class test set and call it testCl.Now build a plsda model using the function plsda() with your training set. plsda.train1 <- plsda(train1, class, ncomp = 10)Use the help() function  for more information on how the function works. After the model is built, visualise it by typing:plotIndiv(plsda.train1, ind.names = TRUE, comp = c(1, 2),          ellipse = TRUE, style = "lattice", cex = c(rep(1, 5)))This will give you a scatterplot of the first two LVs showing the separation between the different groups. The ellipses represent the 95% confidence intervals. What do you observe regarding the separation of the different classes?For datasets with relatively small number of variables, like this one, you can use the vip() function to classify the X-variables according to their explanatory power of Y, for each latent variable. For example:train.vip <- vip(plsda.train1)The VIPs can be visualised with a barplot:ggplot(data = , aes(x = , y = , fill = )) +  #for fill= select the sensor names    labs(x = "Sensor", y = "VIP") +    geom_bar(stat="identity")+    ggtitle()+ # add graph title    theme(legend.position = "none", axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))NOTE: The graph above shows the VIPs for the first LV. Predictors with VIPs >1 are the most relevant for explaining Y. You can also create a correlation plot for the variables by typing:plotVar(plsda.train1, cutoff = 0.7)A cut-off can be set to display only the variables that mostly contribute to the definition of each component. These variables should be located towards the circle of radius 1, far from the centre.You can produce a clustered image map, using the function cim(plsda.train1) to visualise the clustering of the different groups and the different variables.Cross Validation: Before you use the test set to predict the class of the “unknown” samples you can attempt to optimise the model via cross-validation. Use the k-fold method with k=4.  set.seed(2543) perf.plsda <- perf(plsda.train1, validation = "Mfold",                    folds=4, progressBar = FALSE, nrepeat=10)This will provide you with a list of performance metrics for your model, which you can visualise in order to help select the optimum number of LVs. The rule of thumb is usually K-1 where K is the number of classes, but it is worth investigating. To do so, plot the error rate for you model for different latent variables (components) numbers:plot(perf.plsda, col = color.mixo(1:3), sd = TRUE, legend.position = "horizontal")BER in the error plot refers to balanced error rate and is appropriate in case of an unbalanced number of samples per class as it calculates the average proportion of wrongly classified samples in each class, weighted by the number of samples in each class. The distances “max.distance”, “centroids.distance”, “mahalanobis.dist” refers to the “prediction distances” applied to predict the coordinates of an “unknown” sample on the set of H latent components and thus predict its class. If we choose max.distance, we can see that the optimum LV number is around 2, as after that the error decrease is negligible. Model Tuning: To further optimise our plsda model, we can perform tuning which selects the optimal number of components and the optimum number of variables to keep. We first set up a grid of keepX values (number of variables) that will be assessed on each component, one component at a time. list.keepX <- seq(4,24, 2)Similar to above we run 4-fold CV repeated 10 times with a maximum distance prediction defined as above. tune.plsda.train1<-tune.splsda(train1, class, ncomp = 4, validation = "Mfold", folds = 4, dist = 'max.dist', progressBar = FALSE, measure = "BER", test.keepX = list.keepX, nrepeat = 10)                               We can then extract the classification error rate averaged across all folds and repeats for each tested keepX value, the optimal number of components (go to help for more details), the optimal number of variables to select per component which is summarised in a plot where the diamond indicated the optimal keepX value:error <- tune.plsda.train1$error.rate# Find optimal number of components based on t-tests on the error ratencomp <- tune.plsda.train1$choice.ncomp$ncomp ncompThis should return ncomp=2. Next calculate the optimal number of variables for the model:select.keepX <- tune.plsda.train1$choice.keepX[1:ncomp]  select.keepXThis should return 4 variables for each component. Create a plot to visualise the error rate for different number of components and different number of variables (Figure 4).plot(tune.plsda.train1, col = color.jet(4))The diamonds in Fig 4 indicate the optimal number of variables to keep for a given component, selected by which keepX value achieves the lowest classification error rate as determined with a one-sided t?test. The error bars indicate the standard deviation across the repeated, cross-validated folds. Also, note the x axis in Fig 4 is not equidistant! You can now create a new optimised plsda model. Name the final model splsda.train1.opt. splsda.train1.opt <- splsda(train1, class, ncomp = ncomp,                             keepX = select.keepX)Note: PLS-DA main outputs are:* A set of components, also called latent variables. There are as many components as the chosen dimension of the PLS-DA model.* A set of loading vectors, which are coefficients assigned to each variable to define each component. These coefficients indicate the importance of each variable in PLS-DA. Importantly, each loading vector is associated to a particular component. Loading vectors are obtained so that the covariance between a linear combination of the variables from X (the X-component) and the factor of interest Y is maximised.* A list of selected variables from X and associated to each component if sPLS-DA is applied.You can visualise the results by typing:plotIndiv(splsda.train1.opt, ind.names = FALSE, legend=TRUE,          ellipse = TRUE, title="sPLS-DA - final result")Prediction of unknown samples: The final step will be to use the test set to predict the class of the “unknown samples”. test.predict1 <- predict(splsda.train1.opt, test1,                          dist = "max.dist")Extract the predictions from the test.predict1 list.prediction <- test.predict1$class$max.dist[,2]Now create a confusion matrix to see the error rate of your model. Did the model correctly assign the samples? table(factor(prediction, levels = levels(class1)), testCl)3. Multivariate classification using PLS-DA and GC-MS data.This data is much larger than that previously used. It was generated in Matlab. The R.matlab package permits Matlab binary files to be imported into R.3.1 Initial Classification and Visualisation Copy the following lines into your script:rm(list=ls())graphics.off()require(R.matlab)require(matlab)require(mixOmics)require(ptw)require(dplyr)source("pretreat.r")For this part of the practical, you need to source pretreat.r. This script includes the function pretreat() which permits data pre-treatment of the data, namely mean-centring, autoscaling, range-scaling and normalisation. In order to import the Matlab data file NA_BWGT_FAE_CTRL_CD.mat, you need to indicate the data path of the file and then use the readMat() function to read it. FILE="Filepath…/NA_BWGT_FAE_CTRL_CD.mat"DATA = readMat(FILE)The variable DATA is a list and contains within it a collection of matrices and vectors (both numeric and character). Each variable can be extracted as shown:XTIC = DATA$XTICcolnames(XTIC)<-sprintf("X%s",seq(1:ncol(XTIC)))# add colnamesCLASS = as.factor(DATA$CLASS)SAM = as.character(unlist(DATA$SAM))RT = DATA$RTscanThe XTIC variable is a numeric matrix that contains the GC-MS profiles (total ion count (TIC) chromatograms). CLASS is a vector whose length is the same as the number of rows in XTIC and contains the class values: 1 = Healthy control; 2 = patient with Crohn’s disease. SAM contains the sample IDs for each GC-MS profile. RT contains the retention times in minutes (length is equal to the number of columns in XTIC).3.2 Exploratory data analysis with PCABefore you proceed to multivariate classification, perform principal components analysis (PCA) on the XTIC matrix to see whether samples of the same class cluster together and identify potential outliers. You can do this using the pca() function of the mixOmics package. Use ncomp=4, scale=TRUE. Then create a scatterplot using the plotIndiv() function for example:plotIndiv(pca.raw,ind.names=SAM, group=as.factor(CLASS), style=“lattice”, legend=TRUE)From the PCA plot we can see that the two groups cluster very close together and we can also  identify several potential outliers. Before we take any action, we first inspect the TIC chromatograms for RT drift, which is a common cause of batch effect in chromatographic techniques.  Using the matplot() function select two rows at random to overlay the chromatograms. Colour the 1st chromatogram black and the 2nd red. Use the add=TRUE and make use of the legend() function so that the name of each sample appears in the legend. For example:matplot(XTIC[1,],type="l",main="Chromatograms",        xlab="RT", ylab="TIC")matplot(XTIC[23,],type="l",col=2,add=TRUE)leg.txt = c(paste("chromatogram", SAM[1]),           paste("\n chromatogram", SAM[23]))legend("topleft", leg.txt, pch="_", col=c(1,2), bty="n")As you can see, plotting the raw data, reveals a significant RT drift for some of the chromatograms which needs to be dealt with before we build our predictive model. 3.3 Alignment of chromatographic peaksThere are a number of alignment techniques available such as parametric time warping (PTW), dynamic time warping (DTW), correlation optimised warping (COW) and Warp2D which is based on COW. In this practical, we will employ PTW based alignment using the library ptw. The function ptw() in this package can be used to align a single sample to a single reference, several samples to a single reference, and several samples to several references. See the library documentation for more information. In this practical we will use the first chromatogram in the dataset as reference (ref) and align all the other chromatograms (samp) on the reference by using the argument warp.type = “individual”. To do this type the following lines: ref <- XTIC[1,]samp <- XTIC[2:24,]gaschrom.ptw <- ptw(ref, samp, try= FALSE,                 warp.type = "individual",                 verbose = TRUE, optim.crit = "WCC",                  trwdth = 100, init.coef = c(0, 1, 0))In the optimization mode (try = FALSE), the function optimizes the warping coefficients using the chosen criterion (optim.crit can be either "WCC" or "RMS"). For "RMS", the data are smoothed before the optimization, but the quality of the final warping is measured on the unsmoothed data. For "WCC", the warping is performed using trwdth as the triangle width, but the quality of the final solution is measured using trwdth.res. WCC stands for “weighted auto- and cross-correlation measures” and is a suitable measure for the similarity of two patterns when features may be shifted. Identical patterns lead to a wcc value of 1.The init.coef refers to the starting coefficients. The first number is the zeroth-order coefficient (i.e., a constant shift). The default is to start from the identity warping using a quadratic function (c(0, 1, 0)). To inspect the object gaschrom.ptw you created earlier you can type summary(gaschrom.ptw)You can extract the “warped” chromatograms from the list above as follows:XTIC.ptw <- as.matrix(gaschrom.ptw[["warped.sample"]])As you may have noticed this matrix has 23 samples as the reference is not included. You need to add it back to the matrix by typing:XTIC.ptw <- rbind(XTIC[1,], XTIC.ptw)Also, after warping some cells in the new matrix have NA values. We replace these with zeros as follows:library(dplyr)XTIC.ptw <- XTIC.ptw %>% replace(is.na(.), 0) Now plot the new data set to see if the chromatograms align better than before. You should expect some noticeable improvement, although the issue still exists with some of the samples. Next perform PCA to see how the samples cluster after alignment (Figure 7).  This time the PCA reveals two “extreme” outliers (W304_FA_CD and W152_FA_CTRL). Remove these two samples from the aligned dataset and name the new matrix XTIC.ptw.out. Create a new vector with sample names called SAM1 and a new factor for the class name called CLASS1. Then proceed with building a prediction algorithm using PLS-DA as instructed in the following section. 3.4 Multivariate classification of aligned data using PLS-DAThe first step is to split the aligned dataset into a training set and a testing set. The former will be used to build a classification model; the latter will be used to see if the model correctly classifies the samples (you can deduce this from the sample ID).  Create a new data matrix called XTrain.ptw from XTIC.ptw.out but remove the 3rd, 9th, 16th and 19th rows. Now do the same for CLASS1 and SAM1 creating Ctrain.ptw and SAMtrain.ptw respectively.Create a new data matrix called XTest.ptw and fill it with the 3rd, 9th, 16th and 19th row from XTIC.ptw.out. Do the same for CLASS1 and SAM1 creating Ctest.ptw and SAMtest.ptw respectively. Now build a plsda model with the training set using the function plsda() as you did earlier, selecting ncomp=10, and visualise it using the plotIndiv() function. Follow the same workflow from Task 2 to optimise the plsda model by selecting the optimum number of components and the optimal number of variables for each component. For the variable selection grid use:list.keepX <- c(5:10,  seq(20, 300, 10))If the tuning only selects one variable manually set ncomp=2 when creating the optimised model.Finally use the test set to predict the class of the “unknowns”. Use the predict()function with dist=”max.dist” and produce a confusion matrix with the classification results. Optional Task3.5 Explore data pre-treatment Scaling the data prior to building a classification model is a common practice in metabolomics.  You have been supplied with the pretreat() function. This function permits data pre-treatment via six methods: mean-centring (1), autoscaling (2), range-scaling 0 -1 (3), range-scaling -1 to 1 (4), normalisation (5), and pareto scaling (6). If a zero is entered then no scaling is performed. It works by extracting the scaling parameters from the training set and applies them to the test set. For example to perform mean-centre scaling copy the following lines:#Mean-CentreXMC = pretreat(XTrain.ptw,XTest.ptw,1)# Extract the scaled datasetsXTrainMC = XMC$trDATAscaledXTestMC = XMC$tstDATAscaledAfter you perform scaling with the method of your choice build a plsda model following the steps as in Section 3.4. Finally use the test set to predict the class of the “unknowns”. Use the predict()function with dist=”max.dist” and produce a confusion matrix with the classification results. MSc Applied Bioinformatics		ML for Metabolomics1